{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking in a Connectionist Network\n",
      "$$$\n",
      "David S. Touretzky, Carnegie Mellon University\n",
      "$$$\n",
      "TOURETZKY Change Butter e 0 MOT °0 °o DEL O —o INS °0 5 4 \n",
      "3 2 mm \"E\" 1 Modul* Input Butler i A B C D \n",
      "Figure\n",
      "['chunking', 'in', 'a', 'connectionist', 'network', 'david', 's', 'touretzky', 'carnegie', 'mellon', 'university', 'touretzky', 'change', 'butter', 'e', 'mot', 'del', 'o', 'ins', 'mm', 'e', 'modul', 'input', 'butler', 'i', 'a', 'b', 'c', 'd', 'figure', 'part', 'of', 'a', 'connectionist', 'network', 'for', 'applying', 'rewrite', 'mles', 'to', 'strings', 'the', 'symbols', 'from', 'which', 'strings', 'are', 'composed', 'are', 'binary', 'feature', 'vectors', 'the', 'experiment', 'reported', 'here', 'uses', 'a', 'representanon', 'with', 'five', 'phonetic', 'features', 'organized', 'as', 'one', 'group', 'of', 'two', 'features', 'and', 'one', 'of', 'three', 'features', 'in', 'real', 'phonology', 'there', 'are', 'many', 'more', 'features', 'they', 'encode', 'the', 'place', 'and', 'manner', 'of', 'articulation', 'of', 'sounds', 'features', 'within', 'a', 'group', 'are', 'mutually', 'exclusive', 'there', 'are', 'a', 'total', 'of', 'six', 'legal', 'symbols', 'labeled', 'a', 'through', 'f', 'the', 'change', 'buffer', 'panems', 'use', 'an', 'elevenelement', 'code', 'for', 'each', 'segment', 'one', 'for', 'signaling', 'deletion', 'five', 'for', 'describing', 'a', 'mutation', 'and', 'five', 'for', 'specifying', 'an', 'insertion', 'symbols', 'are', 'always', 'inserted', 'to', 'the', 'right', 'of', 'the', 'corresponding', 'input', 'buffer', 'segment', 'change', 'buffer', 'patterns', 'are', 'tristate', 'means', 'no', 'change', 'means', 'turn', 'the', 'corresponding', 'bit', 'in', 'the', 'input', 'buffer', 'on', 'and', 'means', 'turn', 'the', 'corresponding', 'bit', 'off', 'for', 'deletion', 'and', 'insertion', 'operations', 'is', 'treated', 'like', 'zero', 'the', 'use', 'of', 'tristate', 'patterns', 'causes', 'the', 'change', 'buffer', 'units', 'to', 'adopt', 'the', 'no', 'change']\n",
      "Total Tokens: 470534\n",
      "Unique Tokens: 21072\n",
      "Total Sequences: 470483\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "\t# replace '--' with a space ' '\n",
    "\tdoc = doc.replace('--', ' ')\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# remove punctuation from each token\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\ttokens = [w.translate(table) for w in tokens]\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()]\n",
    "\t# make lower case\n",
    "\ttokens = [word.lower() for word in tokens]\n",
    "\treturn tokens\n",
    "\n",
    "# save tokens to file, one dialog per line\n",
    "def save_doc(lines, filename):\n",
    "\tdata = '\\n'.join(lines)\n",
    "\tfile = open(filename, 'w')\n",
    "\tfile.write(data)\n",
    "\tfile.close()\n",
    "\n",
    "# load document\n",
    "in_filename = 'output1989.txt'\n",
    "doc = load_doc(in_filename)\n",
    "print(doc[:200])\n",
    "\n",
    "# clean document\n",
    "tokens = clean_doc(doc)\n",
    "tokens = [''.join([i for i in x if i.isalpha()]) for x in tokens if x != '$$$']\n",
    "print(tokens[:200])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))\n",
    "\n",
    "# organize into sequences of tokens\n",
    "length = 50 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "\t# select sequence of tokens\n",
    "\tseq = tokens[i-length:i]\n",
    "\t# convert into a line\n",
    "\tline = ' '.join(seq)\n",
    "\t# store\n",
    "\tsequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "\n",
    "# save sequences to file\n",
    "out_filename = 'output1989_clean.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from pickle import dump\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# load\n",
    "in_filename = 'output1989_clean.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "lines = lines[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 50)            59100     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50, 128)           91648     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1182)              152478    \n",
      "=================================================================\n",
      "Total params: 451,322\n",
      "Trainable params: 451,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /home/rol044/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/200\n",
      "5000/5000 [==============================] - 21s 4ms/step - loss: 6.4578 - accuracy: 0.0674\n",
      "Epoch 2/200\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 5.9302 - accuracy: 0.0812\n",
      "Epoch 3/200\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 5.8902 - accuracy: 0.0812\n",
      "Epoch 4/200\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 5.8798 - accuracy: 0.0812\n",
      "Epoch 5/200\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 5.8703 - accuracy: 0.0812\n",
      "Epoch 6/200\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 5.8206 - accuracy: 0.0812\n",
      "Epoch 7/200\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 5.7283 - accuracy: 0.0812\n",
      "Epoch 8/200\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 5.6770 - accuracy: 0.0812\n",
      "Epoch 9/200\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 5.6363 - accuracy: 0.0818\n",
      "Epoch 10/200\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 5.6009 - accuracy: 0.0802\n",
      "Epoch 11/200\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 5.5505 - accuracy: 0.0804\n",
      "Epoch 12/200\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 5.4959 - accuracy: 0.0844\n",
      "Epoch 13/200\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 5.4243 - accuracy: 0.0862\n",
      "Epoch 14/200\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 5.3327 - accuracy: 0.0876\n",
      "Epoch 15/200\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 5.2523 - accuracy: 0.0948\n",
      "Epoch 16/200\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 5.1860 - accuracy: 0.0970\n",
      "Epoch 17/200\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 5.1324 - accuracy: 0.0932\n",
      "Epoch 18/200\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 5.0699 - accuracy: 0.0982\n",
      "Epoch 19/200\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 5.0226 - accuracy: 0.1024\n",
      "Epoch 20/200\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 4.9736 - accuracy: 0.1014\n",
      "Epoch 21/200\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 4.9191 - accuracy: 0.1024\n",
      "Epoch 22/200\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 4.8775 - accuracy: 0.1056\n",
      "Epoch 23/200\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 4.8372 - accuracy: 0.1034\n",
      "Epoch 24/200\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 4.7959 - accuracy: 0.1070\n",
      "Epoch 25/200\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 4.7554 - accuracy: 0.1094\n",
      "Epoch 26/200\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 4.7133 - accuracy: 0.1082\n",
      "Epoch 27/200\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 4.6596 - accuracy: 0.1120\n",
      "Epoch 28/200\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 4.6219 - accuracy: 0.1152\n",
      "Epoch 29/200\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 4.5859 - accuracy: 0.1152\n",
      "Epoch 30/200\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 4.5789 - accuracy: 0.1188\n",
      "Epoch 31/200\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 4.6793 - accuracy: 0.1166\n",
      "Epoch 32/200\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 4.5511 - accuracy: 0.1186\n",
      "Epoch 33/200\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 4.4591 - accuracy: 0.1200\n",
      "Epoch 34/200\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 4.3800 - accuracy: 0.1254\n",
      "Epoch 35/200\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 4.3275 - accuracy: 0.1292\n",
      "Epoch 36/200\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 4.2719 - accuracy: 0.1346\n",
      "Epoch 37/200\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 4.2174 - accuracy: 0.1386\n",
      "Epoch 38/200\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 4.1831 - accuracy: 0.1382\n",
      "Epoch 39/200\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 4.1362 - accuracy: 0.1436\n",
      "Epoch 40/200\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 4.1191 - accuracy: 0.1506\n",
      "Epoch 41/200\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 4.0786 - accuracy: 0.1506\n",
      "Epoch 42/200\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 4.0294 - accuracy: 0.1588\n",
      "Epoch 43/200\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 3.9734 - accuracy: 0.1630\n",
      "Epoch 44/200\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 3.9412 - accuracy: 0.1622\n",
      "Epoch 45/200\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 3.8872 - accuracy: 0.1668\n",
      "Epoch 46/200\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 3.8562 - accuracy: 0.1672\n",
      "Epoch 47/200\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 3.8087 - accuracy: 0.1718\n",
      "Epoch 48/200\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 3.7573 - accuracy: 0.1740\n",
      "Epoch 49/200\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 3.7477 - accuracy: 0.1718\n",
      "Epoch 50/200\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 3.6841 - accuracy: 0.1808\n",
      "Epoch 51/200\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 3.6204 - accuracy: 0.1864\n",
      "Epoch 52/200\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 3.5811 - accuracy: 0.1998\n",
      "Epoch 53/200\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 3.5548 - accuracy: 0.1980\n",
      "Epoch 54/200\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 3.5753 - accuracy: 0.2004\n",
      "Epoch 55/200\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 3.5389 - accuracy: 0.2062\n",
      "Epoch 56/200\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 3.4865 - accuracy: 0.2130\n",
      "Epoch 57/200\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 3.4200 - accuracy: 0.2166\n",
      "Epoch 58/200\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 3.4908 - accuracy: 0.2188\n",
      "Epoch 59/200\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 3.4665 - accuracy: 0.2050\n",
      "Epoch 60/200\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 3.4027 - accuracy: 0.2148\n",
      "Epoch 61/200\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 3.3744 - accuracy: 0.2222\n",
      "Epoch 62/200\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 3.3331 - accuracy: 0.2320\n",
      "Epoch 63/200\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 3.2721 - accuracy: 0.2398\n",
      "Epoch 64/200\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 3.2796 - accuracy: 0.2412\n",
      "Epoch 65/200\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 3.1766 - accuracy: 0.2520\n",
      "Epoch 66/200\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 3.0973 - accuracy: 0.2614\n",
      "Epoch 67/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 16s 3ms/step - loss: 3.0414 - accuracy: 0.2648\n",
      "Epoch 68/200\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 2.9738 - accuracy: 0.2822\n",
      "Epoch 69/200\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 2.9339 - accuracy: 0.2814\n",
      "Epoch 70/200\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 2.8912 - accuracy: 0.3012\n",
      "Epoch 71/200\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 2.8711 - accuracy: 0.3016\n",
      "Epoch 72/200\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 2.8004 - accuracy: 0.3130\n",
      "Epoch 73/200\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 2.7527 - accuracy: 0.3324\n",
      "Epoch 74/200\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 2.7504 - accuracy: 0.3302\n",
      "Epoch 75/200\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 2.7298 - accuracy: 0.3292\n",
      "Epoch 76/200\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 2.6682 - accuracy: 0.3438\n",
      "Epoch 77/200\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 2.6217 - accuracy: 0.3526\n",
      "Epoch 78/200\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 2.5874 - accuracy: 0.3632\n",
      "Epoch 79/200\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 2.7564 - accuracy: 0.3476\n",
      "Epoch 80/200\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 2.7689 - accuracy: 0.3316\n",
      "Epoch 81/200\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 2.6525 - accuracy: 0.3524\n",
      "Epoch 82/200\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 2.5866 - accuracy: 0.3544\n",
      "Epoch 83/200\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 2.5044 - accuracy: 0.3766\n",
      "Epoch 84/200\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 2.4532 - accuracy: 0.3850\n",
      "Epoch 85/200\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 2.3948 - accuracy: 0.3936\n",
      "Epoch 86/200\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 2.3353 - accuracy: 0.4078\n",
      "Epoch 87/200\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 2.2857 - accuracy: 0.4182\n",
      "Epoch 88/200\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 2.2466 - accuracy: 0.4308\n",
      "Epoch 89/200\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 2.2417 - accuracy: 0.4388\n",
      "Epoch 90/200\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 2.1641 - accuracy: 0.4512\n",
      "Epoch 91/200\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 2.1293 - accuracy: 0.4562\n",
      "Epoch 92/200\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 2.0895 - accuracy: 0.4626\n",
      "Epoch 93/200\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 2.0428 - accuracy: 0.4754\n",
      "Epoch 94/200\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 2.0092 - accuracy: 0.4908\n",
      "Epoch 95/200\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 1.9796 - accuracy: 0.4860\n",
      "Epoch 96/200\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 1.9499 - accuracy: 0.5048\n",
      "Epoch 97/200\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 1.9283 - accuracy: 0.5058\n",
      "Epoch 98/200\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 1.8850 - accuracy: 0.5156\n",
      "Epoch 99/200\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 1.8672 - accuracy: 0.5208\n",
      "Epoch 100/200\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 1.8330 - accuracy: 0.5304\n",
      "Epoch 101/200\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 1.8581 - accuracy: 0.5250\n",
      "Epoch 102/200\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 1.8658 - accuracy: 0.5242\n",
      "Epoch 103/200\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 1.8134 - accuracy: 0.5476\n",
      "Epoch 104/200\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 1.7280 - accuracy: 0.5596\n",
      "Epoch 105/200\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 1.8262 - accuracy: 0.5556\n",
      "Epoch 106/200\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 1.7884 - accuracy: 0.5584\n",
      "Epoch 107/200\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 1.6434 - accuracy: 0.5894\n",
      "Epoch 108/200\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 1.5792 - accuracy: 0.5944\n",
      "Epoch 109/200\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 1.5548 - accuracy: 0.6122\n",
      "Epoch 110/200\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 1.5200 - accuracy: 0.6154\n",
      "Epoch 111/200\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 1.5047 - accuracy: 0.6248\n",
      "Epoch 112/200\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 1.4815 - accuracy: 0.6344\n",
      "Epoch 113/200\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 1.4664 - accuracy: 0.6248\n",
      "Epoch 114/200\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 1.4128 - accuracy: 0.6448\n",
      "Epoch 115/200\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 1.3796 - accuracy: 0.6500\n",
      "Epoch 116/200\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 1.3215 - accuracy: 0.6708\n",
      "Epoch 117/200\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 1.3033 - accuracy: 0.6724\n",
      "Epoch 118/200\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 1.2952 - accuracy: 0.6802\n",
      "Epoch 119/200\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 1.3029 - accuracy: 0.6744\n",
      "Epoch 120/200\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 1.2497 - accuracy: 0.6934\n",
      "Epoch 121/200\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 1.2366 - accuracy: 0.6960\n",
      "Epoch 122/200\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 1.2449 - accuracy: 0.6916\n",
      "Epoch 123/200\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 1.2202 - accuracy: 0.6992\n",
      "Epoch 124/200\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 1.2466 - accuracy: 0.6974\n",
      "Epoch 125/200\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 1.2920 - accuracy: 0.6810\n",
      "Epoch 126/200\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 1.1666 - accuracy: 0.7070\n",
      "Epoch 127/200\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 1.1346 - accuracy: 0.7116\n",
      "Epoch 128/200\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 1.0799 - accuracy: 0.7346\n",
      "Epoch 129/200\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 1.0641 - accuracy: 0.7396\n",
      "Epoch 130/200\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 1.0285 - accuracy: 0.7446\n",
      "Epoch 131/200\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 1.0108 - accuracy: 0.7544\n",
      "Epoch 132/200\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 1.0127 - accuracy: 0.7544\n",
      "Epoch 133/200\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 1.0115 - accuracy: 0.7502\n",
      "Epoch 134/200\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 1.2613 - accuracy: 0.7210\n",
      "Epoch 135/200\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 1.3562 - accuracy: 0.7012\n",
      "Epoch 136/200\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 1.3063 - accuracy: 0.7046\n",
      "Epoch 137/200\n",
      "5000/5000 [==============================] - 14s 3ms/step - loss: 1.2773 - accuracy: 0.7204\n",
      "Epoch 138/200\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 1.3078 - accuracy: 0.6888\n",
      "Epoch 139/200\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 1.0525 - accuracy: 0.7488\n",
      "Epoch 140/200\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.9586 - accuracy: 0.7748\n",
      "Epoch 141/200\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 0.9058 - accuracy: 0.7854\n",
      "Epoch 142/200\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 0.8351 - accuracy: 0.8068\n",
      "Epoch 143/200\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.8498 - accuracy: 0.8000\n",
      "Epoch 144/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 14s 3ms/step - loss: 0.8295 - accuracy: 0.8042\n",
      "Epoch 145/200\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 0.7913 - accuracy: 0.8180\n",
      "Epoch 146/200\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 0.8419 - accuracy: 0.8102\n",
      "Epoch 147/200\n",
      "5000/5000 [==============================] - 14s 3ms/step - loss: 0.8259 - accuracy: 0.8156\n",
      "Epoch 148/200\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.7835 - accuracy: 0.8250\n",
      "Epoch 149/200\n",
      "5000/5000 [==============================] - 14s 3ms/step - loss: 0.7139 - accuracy: 0.8370\n",
      "Epoch 150/200\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 0.6706 - accuracy: 0.8536\n",
      "Epoch 151/200\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 0.6821 - accuracy: 0.8524\n",
      "Epoch 152/200\n",
      "5000/5000 [==============================] - 14s 3ms/step - loss: 0.6622 - accuracy: 0.8526\n",
      "Epoch 153/200\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.6064 - accuracy: 0.8672\n",
      "Epoch 154/200\n",
      "5000/5000 [==============================] - 14s 3ms/step - loss: 0.5933 - accuracy: 0.8722\n",
      "Epoch 155/200\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.6541 - accuracy: 0.8480\n",
      "Epoch 156/200\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 0.5900 - accuracy: 0.8698\n",
      "Epoch 157/200\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 0.5737 - accuracy: 0.8788\n",
      "Epoch 158/200\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 1.0463 - accuracy: 0.8010\n",
      "Epoch 159/200\n",
      "5000/5000 [==============================] - 14s 3ms/step - loss: 1.2088 - accuracy: 0.7610\n",
      "Epoch 160/200\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.9835 - accuracy: 0.7958\n",
      "Epoch 161/200\n",
      "5000/5000 [==============================] - 14s 3ms/step - loss: 0.9346 - accuracy: 0.7948\n",
      "Epoch 162/200\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 0.8021 - accuracy: 0.8308\n",
      "Epoch 163/200\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.6989 - accuracy: 0.8508\n",
      "Epoch 164/200\n",
      "5000/5000 [==============================] - 14s 3ms/step - loss: 0.6445 - accuracy: 0.8670\n",
      "Epoch 165/200\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.6348 - accuracy: 0.8760\n",
      "Epoch 166/200\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 0.6343 - accuracy: 0.8776\n",
      "Epoch 167/200\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 0.5826 - accuracy: 0.8836\n",
      "Epoch 168/200\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.5567 - accuracy: 0.8860\n",
      "Epoch 169/200\n",
      "5000/5000 [==============================] - 14s 3ms/step - loss: 0.5300 - accuracy: 0.8952\n",
      "Epoch 170/200\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.4853 - accuracy: 0.9038\n",
      "Epoch 171/200\n",
      "5000/5000 [==============================] - 4s 774us/step - loss: 0.4584 - accuracy: 0.9166\n",
      "Epoch 172/200\n",
      "5000/5000 [==============================] - 4s 776us/step - loss: 0.4579 - accuracy: 0.9126\n",
      "Epoch 173/200\n",
      "5000/5000 [==============================] - 12s 2ms/step - loss: 0.5438 - accuracy: 0.9048\n",
      "Epoch 174/200\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 0.6471 - accuracy: 0.8742\n",
      "Epoch 175/200\n",
      "5000/5000 [==============================] - 4s 794us/step - loss: 0.5427 - accuracy: 0.8782\n",
      "Epoch 176/200\n",
      "5000/5000 [==============================] - 4s 788us/step - loss: 0.5819 - accuracy: 0.8656\n",
      "Epoch 177/200\n",
      "5000/5000 [==============================] - 4s 777us/step - loss: 0.5134 - accuracy: 0.8868\n",
      "Epoch 178/200\n",
      "5000/5000 [==============================] - 4s 763us/step - loss: 0.4233 - accuracy: 0.9200\n",
      "Epoch 179/200\n",
      "5000/5000 [==============================] - 4s 737us/step - loss: 0.3709 - accuracy: 0.9306\n",
      "Epoch 180/200\n",
      "5000/5000 [==============================] - 4s 754us/step - loss: 0.3367 - accuracy: 0.9430\n",
      "Epoch 181/200\n",
      "5000/5000 [==============================] - 4s 782us/step - loss: 0.3153 - accuracy: 0.9482\n",
      "Epoch 182/200\n",
      "5000/5000 [==============================] - 4s 758us/step - loss: 0.3080 - accuracy: 0.9496\n",
      "Epoch 183/200\n",
      "5000/5000 [==============================] - 4s 777us/step - loss: 0.2993 - accuracy: 0.9508\n",
      "Epoch 184/200\n",
      "5000/5000 [==============================] - 4s 816us/step - loss: 0.2779 - accuracy: 0.9566\n",
      "Epoch 185/200\n",
      "5000/5000 [==============================] - 4s 770us/step - loss: 0.2599 - accuracy: 0.9582\n",
      "Epoch 186/200\n",
      "5000/5000 [==============================] - 4s 789us/step - loss: 0.2536 - accuracy: 0.9644\n",
      "Epoch 187/200\n",
      "5000/5000 [==============================] - 4s 791us/step - loss: 0.2450 - accuracy: 0.9658\n",
      "Epoch 188/200\n",
      "5000/5000 [==============================] - 4s 766us/step - loss: 0.2271 - accuracy: 0.9686\n",
      "Epoch 189/200\n",
      "5000/5000 [==============================] - 4s 800us/step - loss: 0.2218 - accuracy: 0.9692\n",
      "Epoch 190/200\n",
      "5000/5000 [==============================] - 4s 754us/step - loss: 0.2092 - accuracy: 0.9724\n",
      "Epoch 191/200\n",
      "2176/5000 [============>.................] - ETA: 2s - loss: 0.2178 - accuracy: 0.9665"
     ]
    }
   ],
   "source": [
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# separate into input and output\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())\n",
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=200)\n",
    "\n",
    "# save the model to file\n",
    "model.save('model.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "easier it can concentrate on learning just the policies of the speakers language this paper makes no assumption that policies require explicit symbolic representations in speakers heads rather it shows that chunking can occur even when there is no working memory trace available and new rules cannot be constructed symbolically the\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "\tresult = list()\n",
    "\tin_text = seed_text\n",
    "\t# generate a fixed number of words\n",
    "\tfor _ in range(n_words):\n",
    "\t\t# encode the text as integer\n",
    "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "\t\t# truncate sequences to a fixed length\n",
    "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "\t\t# predict probabilities for each word\n",
    "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
    "\t\t# map predicted word index to word\n",
    "\t\tout_word = ''\n",
    "\t\tfor word, index in tokenizer.word_index.items():\n",
    "\t\t\tif index == yhat:\n",
    "\t\t\t\tout_word = word\n",
    "\t\t\t\tbreak\n",
    "\t\t# append to input\n",
    "\t\tin_text += ' ' + out_word\n",
    "\t\tresult.append(out_word)\n",
    "\treturn ' '.join(result)\n",
    "\n",
    "# load cleaned text sequences\n",
    "in_filename = 'output1989_clean.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "lines = lines[:5000]\n",
    "seq_length = len(lines[0].split()) - 1\n",
    "\n",
    "# load the model\n",
    "model = load_model('model.h5')\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    "\n",
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connectionist chunker acquires its rules incrementally through selfsupervised backpropagation and rehearsal of prior knowledge further experiments are planned to analyze the representations the chunker develops acknowledgments tion grant and by the office of naval research under contract number i thank deirdre wheeler marco zagha michael witbrock and george lakoff for discussions that contributed to this work and gillette elvgren for help with the simulations references newell a the william james lectures unified theories of cognition given at harvard university laird j e newell a and rosenbloom p s soar an architecture for general intelligence artificial intelligence pinker s and prince\n"
     ]
    }
   ],
   "source": [
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 100)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
